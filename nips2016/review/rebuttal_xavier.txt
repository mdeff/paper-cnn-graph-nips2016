2763

We warmly thank the referees for their time and their helpful comments regarding our manuscript. 

MNIST experiments (R1,R3,R7): The accuracy gap between standard CNN and graph CNN can be reduced with more epochs. With 20 epochs (instead of 10 in our submission) and LeNet5 architecture, standard CNN has 99.33% accuracy and graph CNN has 99.14%. This reveals a slower convergence of graph CNN, and future work will investigate better optimization strategies.

R1: Thanks for pointing out the GNN model (we will add this ref). GNN processes data in the spatial/graph domain, unlike our model that is defined in the spectral domain. It is not direct to formulate GNN in spectral form (and therefore to compare both techniques), that is, how to define spectral filters g s.t. Psi(x,y)=g(L)x (not only identity filter g(z)=z)? 
The spectral filters are indeed rotationally invariant. We think that this property might actually be an advantage for large-scale image datasets, where same objects may rotate, and thus requiring standard CNN to learn more filters than graph CNN. We will explore this idea in future work.

R7: We indeed consider a single graph for this work, which is already meaningful for multiple applications s.a. brain connectivity analysis, and social network analysis (one graph and many data signals on this graph). Extension to multiple graphs is obviously of great interest, and can also be explored with a spectral technique like the one proposed here. We reserve this extension for another paper. 
The pooling strategy is inevitable in any CNN architecture as it allows to reduce spatial resolution and to increase filter resolution. In the case of grids, it is mathematically well-defined, but in case of graphs it is equivalent to graph clustering, which is NP-hard. We opted here for the Graclus model, which has proved to be a very efficient graph clustering technique.
It is expected that accuracy drops (3%) when the graph structure is destroyed with a random graph, as convolutional layers become meaningless. 
For 20NEWS, we compare against state-of-the-art Multinomial NB technique, and other baseline techniques. 

R8: We have recently improved the presentation, accessibility, and Section 4 of the paper. We will be happy to release it.
When random graphs are used, then the convolutional layers become meaningless. However, the last part of the CNN system, the fully connected layers stay active, and absorbs the drop in performance. 
There are plenty of theoretical works in the area of spectral analysis, see e.g. the review [29].
The complexity of our technique is O(EK), which reduces to linear O(n) in case of sparse graphs, vs O(n^2) for [13].

R9: For the presentation of the paper, see our answer to R8.
We did not present any natural graph experiments due to the space limitation. 
