\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

%\usepackage{nips_2016}
\usepackage[final]{nips2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[english]{babel}
\usepackage[colorlinks=true,allcolors=blue]{hyperref}       % hyperlinks
%\usepackage{url}            % simple URL typesetting
%\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
%\usepackage{amssymb}
\usepackage{xfrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
\usepackage{subfigure}
\usepackage{graphicx}
%\usepackage{tabularx}
\usepackage{booktabs}
%\usepackage{multirow}
%\usepackage{svg}
\usepackage{setspace}
\usepackage{color}

% Use less space
%\usepackage{titlesec}		% space before / after headings
%\titlespacing\section{0pt}{10pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
%\titlespacing\subsection{0pt}{10pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
%\titlespacing\subsubsection{0pt}{10pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
%\usepackage{parskip} \setlength{\parskip}{5pt}
\setlength{\bibsep}{0.2pt}

\title{Convolutional Neural Networks on Graphs\\
with Fast Localized Spectral Filtering}

\author{
  Michaël Defferrard \\ %\thanks{further info} \\
  %Department of Electrical Engineering \\
  %École Polytechnique Fédérale de Lausanne (EPFL) \\
  EPFL, Lausanne, Switzerland \\
  \texttt{michael.defferrard@epfl.ch} \\
  \And % \AND
  Xavier Bresson \\
  EPFL, Lausanne, Switzerland \\
  \texttt{xavier.bresson@epfl.ch} \\
  \And % \AND
  Pierre Vandergheynst \\
  EPFL, Lausanne, Switzerland \\
  \texttt{pierre.vandergheynst@epfl.ch} \\
}

\usepackage[acronym]{glossaries}
\newacronym{CNN}{CNN}{Convolutional Neural Network}
\newacronym{SVD}{SVD}{Singular Value Decomposition}
\newacronym{SGD}{SGD}{Stochastic Gradient Descent}
\newacronym{PSD}{PSD}{positive semidefinite}

\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\spn}{span}
\newcommand{\G}{\mathcal{G}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\bO}{\mathcal{O}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\eqnref}[1]{(\ref{eq:#1})}
\newcommand{\bruna}{art:BrunaZarembaSzlamLeCun13DLgraphs,
art:HenaffBrunaLeCun15DLgraphs}

\newcommand{\todo}[1]{{\color{red} #1 }}

\begin{document}

\maketitle

\begin{abstract}

	Convolutional neural networks (CNNs) have greatly improved state-of-the-art
	performances in a number of fields, notably computer vision and natural
	language processing. In this work, we are interested in generalizing the
	formulation of CNNs from low-dimensional regular Euclidean domains, where
	image (2D), video (3D) and audio (1D) are represented, to high-dimensional
	irregular domains represented by graphs. This paper introduces a formulation
	of CNNs on graphs in the context of spectral graph theory. We borrow the
	fundamental tools from the emerging field of signal processing on graphs,
	which provides the necessary mathematical background and efficient numerical
	schemes to design localized graph filters which are fast to evaluate.
	Similarly to classical CNNs, the computational complexity as well as the
	number of parameters to learn are independent of the data dimensionality.
	Numerical experiments on MNIST and 20NEWS demonstrate the ability of the
	model to learn local and stationary features on graphs, as long as the graph
	is well-constructed. This work can be thought as the foundation of a second
	generation of CNNs on graphs built on spectral graph theory, being
	mathematically well defined, bringing better test performance and being
	faster to evaluate and train than the first generation introduced by Bruna,
	LeCun et al.

	%there is nothing to lose by going to the graph domain
	%except the rotation invariance

	%Graphs can either be the natural domain of the data, such as social networks,
	%biological graphs like gene regulatory and brain connectivity networks,
	%telecommunication networks, or constructed from the data, such as similarity
	%graphs.
	
	%not a generalization because it provides more invariance, but we don't want
	%to say it.
	%Our formulation is not a direct generalization of classical CNNs, in the
	%sense that it is not a spatial construction.
\end{abstract}

%Previous works have been developed along this today essential line of research, but they lack of mathematical foundations and efficient learning of filters.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{images/illustrationCNNgraphs.eps}
%\includegraphics[width=13.5cm]{images/illustrationCNNgraphs.png}
\caption{Architecture of the proposed CNNs on graphs. Notation: $l$ is the
coarsening level, $x^l$ are the downsampled signals at layer $l$, $\G^l$ is the coarser graph, $g_{\theta^{K_l}}$ are the spectral filters at layer $l$, $x_g^l$ are the filtered signals, $p_l$ is the coarsening exponent, $n_c$ is the number of classes, $y$ is the output signal, and $\theta^l$ is the number of parameters to learn at $l$.}
\label{fig:}
\end{figure}

\section{Introduction}

Convolutional neural networks (CNNs) \cite{pro:LeCunBottouBengioHaffner98MNIST}
offer an efficient architecture to extract highly meaningful statistical
patterns in large-scale and high-dimensional datasets. The key success of CNNs
is its advanced ability to learn local stationary structures and compose them to
form multi-scale hierarchical patterns. Precisely, CNNs extract the local
stationarity property of the input data or signals by revealing local features
that are shared across the data domain. These similar features are identified
with localized convolutional filters or kernels, which are learned from the
data. Convolutional filters are shift- or translation-invariant filters, meaning
they are able to recognize identical features independently of their spatial
locations. Localized kernels or compactly supported filters refer to filters
that extract local features independently of the input data size, with a support
size that can be much smaller than the input size.

The compositional local stationarity property of CNNs can be efficiently
implemented on regular grids, like image and sound domains, as convolutional and
downsampling operators are mathematically well-defined on such discretized
Euclidean spaces. This has led to the breakthrough of CNNs in image, video, and
sound recognition tasks \cite{art:LeCunBengioHinton15DL} as these data lie on
low-dimensional regular lattices. But not all data lie on regular grids. User
data on social networks, gene data on biological regulatory networks, log data
on telecommunication networks, or text documents on words' embedding are
important examples of data lying on irregular or non-Euclidean domains.
Unfortunately, CNNs cannot be directly applied to such complex domains.

A generalization to irregular grids is not direct as the building blocks of
CNNs, the convolution and downsampling operators, are not mathematically clear
on these domains. This makes this extension challenging, both theoretically and
implementation-wise. This work leverages the recent works of
\cite{pro:GregorLeCun10LRF, pro:CoatesNg11LRF,
art:BrunaZarembaSzlamLeCun13DLgraphs, art:HenaffBrunaLeCun15DLgraphs,
art:HammondVandergheynstGribonval11GraphWav,
art:ShumanNarangFrossardOrtegaVandergheynst13ReviewSPG} to introduce an
efficient implementation of CNNs on irregular domains represented here as
graphs. Graphs are universal representations of heterogeneous pairwise
relationships between possibly high-dimensional data. Graphs can encode complex
geometric structures, and can be studied with strong mathematical tools such as
spectral graph theory, a blend between graph theory and harmonic analysis.

The major bottleneck of generalizing CNNs to graphs, and the primary goal of
this work, is the definition of localized filters which are efficient to
evaluate and learn.
%that are the core elements to extract local stationarity structures.
We will make use of recent tools developed in the context of graph signal
processing (GSP) \cite{art:ShumanNarangFrossardOrtegaVandergheynst13ReviewSPG}
to precisely control the spatial support of graph filters as well as to allow
fast evaluations. Eventually, a generic and efficient implementation of CNNs for
graph signals may pave the way to the development of a potentially large class
of deep learning techniques for graph-based data.

% Why convolution? Weight sharing, subject to assumptions
%\paragraph{Low learning complexity.} Learning deep fully connected networks is
%almost computationally intractable for large data domain.  Fortunately, for
%data satisfying stationarity and hierarchical compositionality structure, CNNs
%have proven to be remarkably powerful to identify highly informative features
%using low learning complexity. For example, learning a single layer with $n$
%input coordinates and $m<n$ outputs require $\bO(n^2)$ complexity to learn fully
%connected networks. It also requires the complexity $\bO(n)$ to learn any
%arbitrary filter, and $\bO(S)$ for localized filters, where $S$ is the size of
%the filter support, independent of the input size $n$. One of our main
%contributions is to stay with the same low complexity for learning graph
%convolutional filters. We will show it is actually possible to keep exactly the
%same complexity as standard CNNs, i.e. $\bO(S)$ for localized graph filters.

\todo{spatial approaches provide localization by definition}
\todo{why spectral approach?}
While a spatial approach is conceivable, it faces the challenge of matching
local neighborhoods, as shown by \cite{art:BrunaZarembaSzlamLeCun13DLgraphs}.

\todo{Another challenge is the definition of a pooling layer.}
Local stationarity property of data is extracted via localized convolutional
kernels. In this section, we are interested to extract the multi-scale
hierarchical composition property of data. This is efficiently achieved via grid
subsampling or downsampling and pooling, which trades spatial resolution with
feature resolution reducing the learning complexity without compromising the
system performances. In the case of standard CNNs for data lying on regular
grids, the subsampling operation is mathematically sound. The translation
operator commutes with the downsampling operator, hence not affecting the
translation invariance property of the system at different scales of grid
resolutions.

\paragraph{Graph Signal Processing.} GSP is an emerging field that aims at
bridging the gap between signal processing techniques like wavelet analysis and
graph theory such as spectral graph analysis
\cite{art:BelkinNiyogi05LaplaBeltrami, art:VonLuxburg07Tutorial}. A goal is to
generalize fundamental analysis operations for signals from regular grids to
irregular structures embodied by graphs. We refer the reader to
\cite{art:ShumanNarangFrossardOrtegaVandergheynst13ReviewSPG}
\todo{others for politics} for an
introduction of the field. Standard operations on grids such as convolution,
translation, filtering, dilatation, modulation or downsampling do not extend
directly to graphs and thus require new mathematical definitions while keeping
the original intuitive concepts. In this context, the authors of
\cite{art:HammondVandergheynstGribonval11GraphWav, art:CoifmanLafon06DifMap,
pro:GavishNadlerCoifman10GraphHaar} revisited the construction of wavelet
operators on graphs. In \cite{art:ShumanFarajiVandergheynst16PyramTrans,
art:RamEladCohen11TreeWavelets}, the authors designed a technique to perform
mutli-scale pyramid transforms on graphs. The works of
\cite{pro:TsitsveroBarbarossa15Uncert, pro:PasdeloupAlamiGriponRabbat15Uncert,
art:PerraudinRicaudShumanVandergheynst16Uncert} redefined uncertainty principles
on graphs, and showed that intuitive concepts may be lost, but can also produce
enhanced localization principles for signals on graphs. In
\cite{pro:HammondRaoaroorJacquesVandergheynst10LassoGraWav}, it was shown how to
carry out lasso-based signal regularization on graphs, and studied the
intertwined relationships between smoothness and sparsity on graphs. In
\cite{pro:TremblayPuyGribonvalVandergheynst16CompSpecClus}, the authors
investigated compressed sensing recovery conditions for graph spectral signal
analysis.

\todo{Model statistical assumptions: locality, stationarity, compositionality.}
These assumptions can be experimentally verified for any dataset. While we know
for them to be true for audio, images and videos, we show experimentally that
they might be satisfied for text documents as long as the graph is properly
constructed.
We also observe that
local stationarity is an assumption that is made about the data, and the CNNs
are fundamentally designed on this data hypothesis. However, we believe this
stationarity assumption to be present in many real-world data, as structured
data are usually composed of repetitive small patterns at different scales of
observation.

\section{Related Work}

%Extending the success of CNNs to
%non-Euclidean domains is essential to boost the power analysis of data lying on
%complex networks like biological, social or telecommunication networks.

Extending CNNs to non-Euclidean domains is a recent line of work. A first
attempt is the so-called local reception fields \cite{pro:GregorLeCun10LRF,
pro:CoatesNg11LRF}, successfully applied to image recognition. The main idea is
to group together features based upon a measure of similarity such as to select
a limited number of connections between two successive layers. While this model
reduces the number of parameters by exploiting the locality assumption, it does
not attempt to exploit any weight-sharing strategy.
%stationarity This approach is however not able to extract similar groups over
%the data domain, as it is not clear how to define convolutional filters on
%these groups.
\todo{read papers}

ShapeNet \cite{pro:MasciBoscainiBronsteinVandergheynst15GeoDL,
art:MasciBoscainiBronsteinVandergheynst15ShapeNet} is a generalization of CNNs
to 3D-meshes, a class of smooth low-dimensional non-Euclidean spaces. The
authors used geodesic polar coordinates to define mesh patches and formulated a
deep learning architecture which allows comparison across different manifolds.
They obtained state-of-the-art results for shape recognition tasks.

In \cite{pro:ChenChengMallat14deepHaar} and \cite{pro:RustamovGuibas14deepHaar},
the authors investigated the construction of Haar wavelet transforms on graphs
using a deep hierarchical architecture. They applied the method to object
recognition on sphere, and to sparse reconstruction of faces.
\todo{Learn parameters of a wavelet kernel? Then it is a parametric method,
they impose a shape to the filters.}
\todo{Unknown graph geometry?}

Finally, \cite{art:BrunaZarembaSzlamLeCun13DLgraphs} and
\cite{art:HenaffBrunaLeCun15DLgraphs} introduced a generalization of CNNs on
graphs using graph counterparts of grid convolution and downsampling. They were
able to learn convolutional filters on graphs using the convolution theorem
\cite{book:Mallat99wavelets} that defines convolutions as linear operators that
diagonalize in the Fourier basis (represented by the eigenvectors of the
Laplacian operator). They were thus the first to introduce a spectral
formulation to extend CNNs on graphs. They also introduced a strategy to learn
the graph structure from the data, and applied their model on image recognition,
text categorization and bioinformatics. This approach does however not scale due
to the necessary multiplications by the graph Fourier basis, a matrix of $n^2$
coefficients, where $n$ is the data dimensionality. Despite the cost of
computing this matrix, which requires an EVD on the graph Laplacian, the
dominant cost is the need to multiply the data by this matrix twice (forward and
inverse Fourier transforms) at a cost of $\bO(n^2)$ operations per forward and
backward pass, a computational bottleneck already identified by the authors.
Besides, as they rely on smoothness (via a spline parametrization) in the
Fourier domain to bring localization in the vertex domain, their model does not
provide a precise control over the local support of their kernels, which is
essential to learn localized filters. This work directly builds on theirs, and
we will show how we overcame these limitations and what we further gained.

\subsection{Contributions}

Our contributions can be summed up as follows:
\begin{enumerate}
%\setlength{\itemsep}{0pt} \setlength{\parskip}{0pt}
\item \textbf{Spectral formulation.} A clean spectral formulation built on
	established tools in GSP.
\item \textbf{Strictly localized filters.} Unlike \cite{\bruna}, the proposed spectral
	filters are strictly localized in a ball of radius $K$, i.e. $K$ hops from
	the central vertex.
\item \textbf{Low computational complexity.} The evaluation of our filters is
	linear w.r.t. the filters support's size $K$ and the number of edges $n \leq
	|\E| \leq n^2$. As most real-world graphs are highly sparse, we have $|\E|
	\ll n^2$ and $|\E| = kn$ for the widespread $k$-neighbors graphs. Moreover,
	this method avoids the Fourier basis altogether, thus the expensive
	eigenvalue decomposition (EVD) necessary to compute it as well as the need
	to store the basis, a matrix of size $n^2$. That is especially relevant when
	working with limited GPU memory. Besides the data, our method only requires
	to store the Laplacian, a sparse matrix of $|\E|$ non-zero values.
\item \textbf{Efficient pooling.} We propose an efficient pooling strategy on
	graphs which, after a rearrangement of the vertices, is analog to pooling of
	1D signals.
\item \textbf{Experimental results.} We present many experiments that ultimately
	show that our formulation is (1) a useful model, (2) computationally
	efficient and (3) superior both in accuracy and complexity to the previous
	generation of spectral graph CNNs introduced in \cite{\bruna}. We also show that
	our formulation performs similarly as a classical CNN on MNIST and study the
	impact of various graph constructions on performance.
\end{enumerate}

Beyond this work, our long-term goal is to make use of GSP tools as much as
possible to formulate mathematically well-designed techniques for CNNs on
graphs. This paper introduces the mathematical and computational foundations of
such models, while future developments may consider to enhance the building
blocks, namely graph filtering and coarsening. In the future, we expect to
further benefit from newly developed tools in GSP to enhance our formulation of
CNNs on graphs.

\section{Learning Fast Localized Spectral Filters} \label{sec:filters}

As there is no notion of spatial translation on graphs, convolutions are
implemented as multiplications in the spectral domain
\cite{art:ShumanNarangFrossardOrtegaVandergheynst13ReviewSPG}. While a spatial
convolution on an Euclidean domain is localized by definition (via the finite
size of the kernel) and has a linear computational complexity $\bO(n)$, a filter
defined in the spectral domain is not localized by default and costs $\bO(n^2)$
operations because of the necessary multiplication by the graph Fourier basis.
Note that while classical CNNs can be speed-up by leveraging the Fourier basis
(instead of computing the convolutions in the spatial domain)
\cite{mathieu_fast_2013}, it is only advantageous because of the $\bO(n \log(n))$
complexity of the FFT, which has no equivalent on graphs (yet). A good choice of
filter parametrization however allows us to prove that when translated (via a
convolution with a delta) to a given center vertex, the filter is strictly
localized in a ball of radius $K$ around that center vertex. Moreover, a
recursive formulation allows the filtering operation to scale linearly with $n$.

\paragraph{Graph Fourier Transform.} We are interested in processing signals
defined on undirected and connected graphs $\G=(\V,\E,W)$, where $\V$ is a
finite set of vertices with $|\V|=n$, $\E$ is a set of edges and $W$ is a
weighted adjacency matrix encoding the connection weight between two vertices. A
signal $x: \V \rightarrow \R$ defined on the nodes of the graph may be regarded
as a vector $x \in \R^n$ where $x_i$ is the value of $x$ at the $i^{th}$ node.
An essential operator in spectral graph analysis is the graph Laplacian
\cite{book:Chung97Spectral}, which combinatorial definition is $L = D - W \in
\R^{n \times n}$ where $D \in \R^{n \times n}$ is the diagonal degree matrix
with $D_{ii} = \sum_j W_{ij}$, and its normalized definition is $L = I_n -
D^{-1/2} W D^{-1/2}$ where $I_n$ is the identity matrix. As $L$ is a real
symmetric positive semidefinite matrix, it has a complete set of orthonormal
eigenvectors $\{u_l\}_{l=0}^{n-1} \in \R^n$, known as the graph Fourier modes,
and their associated ordered real nonnegative eigenvalues
$\{\lambda_l\}_{l=0}^{n-1}$, identified as the frequencies of the graph. The
Laplacian is indeed diagonalized by the Fourier basis $U=[u_0, \ldots, u_{n-1}]
\in \R^{n \times n}$ such that $L = U \Lambda U^T$ where $\Lambda =
\diag([\lambda_0, \ldots, \lambda_{n-1}]) \in \R^{n \times n}$. We can now
define the graph Fourier transform (GFT) of a spatial signal $x \in \R^n$ as
$\hat{x} = U^T x \in \R^n$, and its inverse as $x = U \hat{x}$
\cite{art:ShumanNarangFrossardOrtegaVandergheynst13ReviewSPG}. Similarly to the
Fourier transform on Euclidean spaces, the GFT enables the formulation of
fundamental operations such as filtering.

\paragraph{Spectral filtering of graph signals.} As we cannot express a
meaningful translation operator in the vertex domain, the convolution operator
on graph $\ast_\G$ is defined in the Fourier domain such that $x \,\ast_\G\, y =
U((U^Tx) \odot (U^Ty))$, where $\odot$ is the element-wise Hadamard product. It
follows that a signal $x$ is filtered by $g_\theta$ as
\begin{equation}
	y = g_\theta(L) x = g_\theta(U \Lambda U^T) x = U g_\theta(\Lambda) U^T x.
\end{equation}
A non-parametric filter, i.e. a filter whose parameters
are all free, would be defined as
\begin{equation} \label{eq:filt_non-param}
	g_\theta(\Lambda) = \diag(\theta),
\end{equation}
where the parameter $\theta \in \R^n$ is a vector of Fourier coefficients.

\paragraph{Polynomial parametrization for localized filters.} There are however
two problems with non-parametric filters: (1) they are not localized and (2)
they are of learning complexity $\bO(n)$, the dimensionality of the data. We are
going to solve those two problems together by reducing this complexity to
$\bO(K)$, the support's size of the filter, exactly as it is for standard CNNs.
While \cite{\bruna} solves it by parameterizing $g_\theta(\Lambda)$ as a cubic
B-spline defined by $K$ control points, let us parametrize it as a polynomial
function
\begin{equation} \label{eq:filt_poly}
	g_\theta(\Lambda) = \sum_{k=0}^{K-1} \theta_k \Lambda^k,
\end{equation}
where the parameter $\theta \in \R^K$ is a vector of polynomial coefficients.
The value at vertex $j$ of the filter $g_\theta$ centered at vertex $i$ is given
by $(g_\theta(L) \delta_i)_j = (g_\theta(L))_{i,j} = \sum_k \theta_k
(L^k)_{i,j}$, where the kernel is translated via a convolution with a Kronecker
delta $\delta_i \in \R^n$. By \cite[Lemma
5.2]{art:HammondVandergheynstGribonval11GraphWav}, $d_\G(i,j) > K$ implies
$(L^K)_{i,j} = 0$, where $d_\G$ is the shortest path distance, i.e. the distance
between two vertices is the minimum number of edges in any path connecting them.
Consequently, spectral filters represented by $K^\text{th}$-order polynomials of
the Laplacian are exactly $K$-localized.

% While there is many possible choice of recursive polynomials
\paragraph{Recursive formulation for fast filtering.} While we have shown how to
learn localized filters with $K$ parameters, the cost to filter a signal $x$ as
$y = Ug_\theta(\Lambda)U^Tx$ is still of $\bO(n^2)$ operations because of the
multiplications with the Fourier basis $U$. A way out is to parametrize
$g_\theta(L)$ as a polynomial function that can be computed recursively from
$L$, as $K$ multiplications by a sparse $L$ costs $\bO(K|\E|) \ll \bO(n^2)$. One
such polynomial, traditionally used in GSP to approximate kernels (like
wavelets), is the Chebyshev expansion
\cite{art:HammondVandergheynstGribonval11GraphWav}. Another option, the Lanczos
algorithm \cite{art:SusnjaraPerraudinKressnerVandergheynst15Lanczos}, which
constructs an orthonormal basis of the Krylov subspace $\mathcal{K}_K(L,x) =
\spn\{x,Lx,\ldots,L^{K-1}x\}$, seems attractive because of the coefficients'
independence. It is however more convoluted and thus left as a future work.

Recall that the Chebyshev polynomial $T_k(x)$ of order $k$ may be generated by
the stable recurrence relation $T_k(x) = 2x T_{k-1}(x) - T_{k-2}(x)$ with $T_0 =
1$ and $T_1 = x$. These polynomials form an orthogonal basis for $L^2([-1,1], dy
/ \sqrt{1-y^2})$, the Hilbert space of square integrable functions with respect
to the measure $dy/\sqrt{1-y^2}$. A filter can thus be parametrized as a
truncated expansion of order $K-1$ such that
\begin{equation} \label{eq:filt_cheby}
	g_\theta(\Lambda) = \sum_{k=0}^{K-1} \theta_k T_k(\tilde{\Lambda}),
\end{equation}
where the parameter $\theta \in \R^K$ is a vector of Chebyshev coefficients and
$T_k(\tilde{\Lambda}) \in \R^{n \times n}$ is the Chebyshev polynomial of order
$k$ evaluated at $\tilde{\Lambda} = 2 \Lambda / \lambda_{max} - I_n$, a diagonal
matrix of scaled eigenvalues that lie in $[-1,1]$.

The filtering operation can then be written as $y = g_\theta(L) x
= \sum_{k=0}^{K-1} \theta_k T_k(\tilde{L}) x$, where $T_k(\tilde{L}) \in \R^{n
\times n}$ is the Chebyshev polynomial of order $k$ evaluated at the scaled
Laplacian $\tilde{L} = 2 L / \lambda_{max} - I_n$. Note that the spectrum of the
normalized Laplacian is bounded by $2$ \cite{book:Chung97Spectral}, such that
the scaling can simply be $\tilde{L} = L - I_n$. Denoting $\bar{x}_k =
T_k(\tilde{L})x \in \R^n$, we can use the recurrence relation to compute
$\bar{x}_k = 2\tilde{L} \bar{x}_{k-1} - \bar{x}_{k-2}$ with $\bar{x}_0 = x$ and
$\bar{x}_1 = \tilde{L}x$. The entire filtering operation
\begin{equation}
	y = g_\theta(L) x = [\bar{x}_0, \ldots, \bar{x}_{K-1}] \theta
\end{equation}
then costs $\bO(K|\E|))$ operations.

\paragraph{Learning filters.} The $j^\text{th}$ output feature map of the sample
$s$ is given by
\begin{equation} \label{eq:filterbank}
	y_{s,j} = \sum_{i=1}^{F_{in}} g_{\theta_{i,j}}(L) x_{s,i} \in \R^n,
\end{equation}
where the $x_{s,i}$ are the input feature maps and the $F_{in} \times F_{out}$ vectors of Chebyshev
coefficients $\theta_{i,j} \in \R^K$ are the layer's trainable parameters. When
training multiple convolutional layers with the backpropagation algorithm, one
needs the two gradients
\begin{align}
	\frac{\partial E}{\partial \theta_{i,j}} =
	\sum_{s=1}^S [\bar{x}_{s,i,0}, \ldots, \bar{x}_{s,i,K-1}]^T
	\frac{\partial E}{\partial y_{s,j}}
	&& \text{and} &&
	\frac{\partial E}{\partial x_{s,i}} =
	\sum_{j=1}^{F_{out}} g_{\theta_{i,j}}(L)
	\frac{\partial E}{\partial y_{s,j}},
\end{align}
where $E$ is the loss energy over a mini-batch of $S$ samples. Each of the
above three computations boils down to $K$ sparse matrix-vector multiplications
and one dense matrix-vector multiplication for a cost of $\bO(K |\E| F_{in}
F_{out} S)$ operations. These can be efficiently computed on parallel
architectures by leveraging tensor operations. As an optimization,
$[\bar{x}_{s,i,0}, \ldots, \bar{x}_{s,i,K-1}]$ can be computed once.

\section{Efficient Pooling on Graphs} \label{sec:pooling}

% Why are we introducing this new pooling definition?

\todo{Why? Bruna uses an ad-hoc technique.}

\paragraph{Graph Coarsening.} In contrast with regular domains, to reduce the
sampling of a graph signal, we need to construct meaningful neighborhoods on
graphs where similar vertices may be clustered together. Doing this for multiple
layers is equivalent to construct a multi-scale clustering of the graph that
preserves local geometric structures. It is however well-known that graph
clustering is a NP-hard problem \cite{art:BuiJonesGraphPartNPhard} and that
approximations must be used. While there exist many clustering techniques (e.g.
the popular spectral clustering \cite{art:VonLuxburg07Tutorial}), we are most
interested in multilevel clustering algorithms, where each level produces a
\textit{coarser} graph which corresponds to the data domain seen at a different
\textit{resolution}. Moreover, clustering techniques that reduce the size of the
graph by a factor two at each level offers a precise control on the coarsening
and pooling size. In this work, we make use of the coarsening phase of the
Graclus multilevel clustering algorithm \cite{art:DhillonGuanKulis07Graclus},
which has been shown to be extremely efficient at clustering a large variety of
graphs. We briefly review it below. Algebraic multigrid techniques on graphs
\cite{art:RonSafroBrandt11MultigridGraph} and the Kron reduction
\cite{art:ShumanFarajiVandergheynst16PyramTrans} are two methods worth exploring
in future works. The later has the main advantage to preserve the ordering of
the Laplacian spectrum, and may be able to commute with the graph convolution
operator. \todo{If proven, this would be a very essential mathematical property
of CNNs on graphs, exactly as downsampling for classical CNNs.}

Graclus \cite{art:DhillonGuanKulis07Graclus}, similarly to METIS, uses a greedy
algorithm to compute successive coarser versions of a given graph and is able to
minimize several popular spectral clustering objectives. We chose the normalized
cut \cite{art:ShiMalik00NCut}, which is an excellent clustering energy.
Graclus' greedy rule consists, at any given coarsening level, in randomly
pickling an unmarked vertex $i$ and matching it with one of its unmarked
neighbors $j$ that maximizes the local normalized cut $W_{ij} (1/d_i + 1/d_j)$.
The two matched vertices are then marked and the coarsened weights are set as
the sum of their weights. The matching is repeated until all nodes have been
explored. This is an extremely fast coarsening scheme which divides the number
of nodes by approximately two (there exist a few singletons, non-matched nodes)
from one level to the next coarser level.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{images/coarsening_crop}
\caption{\textbf{Example of Graph Coarsening and Pooling.} Let's say we want to
perform a max pooling of size 4 (or two poolings of size 2) on a signal $x \in
\R^8$ living on $\G_0$, the finest graph given as input. Note that it originally
possesses $n_0 = |\V_0| = 8$ vertices, arbitrarily ordered. For a pooling of
size 4, two coarsenings of size 2 are needed: Graclus first gives $\G_1$ of size
$n_1 = |\V_1| = 5$, then $\G_2$ of size $n_2 = |\V_2| = 3$, the coarsest graph.
Sizes are thus set to $n_2 = 3$, $n_1 = 6$, $n_0 = 12$ and fake nodes (in blue)
are added to $\V_1$ (1 node) and $\V_0$ (4 nodes) to pair with the singeltons
(in orange), such that each node has exactly two children.  Nodes in $\V_2$ are
then arbitrarily ordered and nodes in $\V_1$ and $\V_0$ are ordered
consequently.  At that point the arrangement of vertices in $\V_0$ permits a
regular 1D pooling on $x \in \R^{12}$ such that $z = [\max(x_0,x_1),
\max(x_4,x_5,x_6), \max(x_8,x_9,x_{10})] \in \R^3$, where the signal components
$x_2,x_3,x_7,x_{11}$ are set to a neutral value.}
\label{fig:pooling}
\end{figure}

\paragraph{Pooling of graph signals.} After coarsening, each node has either two
parents, if it was matched at the finer level, or one, if it was not, i.e the
node was a singelton. From the coarsest to finest level, fake nodes, i.e.
disconnected nodes, are added to pair with the singletons such that each node
has two children. This structure can be seen as a balanced binary tree: (1)
regular nodes (and singletons) either have two regular nodes (e.g. level 1
vertex 0 in \figref{pooling}) or (2) one singleton and a fake node as children
(e.g. level 2 vertex 0 in \figref{pooling}), and (3) fake nodes always have two
fake nodes as children (e.g. level 1 vertex 1 in \figref{pooling}). Input
signals are initialized with a neutral value at the fake nodes, e.g. 0 when
using a ReLU activation with max pooling. Because these nodes are disconnected,
filtering won't impact the initial neutral value. While those fake nodes do
artificially increase the dimensionality thus the computational cost, we found
that, in practice, the number of singletons left by Graclus is quite low
% < 10%?
compared to the number of vertices.

Arbitrarily ordering the nodes at the coarsest level, then propagating this
ordering to the finest levels, i.e. node $k$ has nodes $2k$ and $2k+1$ as
children, produces a regular ordering in the finest level. Regular in the sense
that adjacent nodes are hierarchically merged at coarser levels. Pooling such a
rearranged graph signal is analog to pooling a regular 1D signal.
\figref{pooling} shows an example of the whole coarsening and pooling process.
This regular arrangement makes the pooling operation very efficient and
satisfies parallel architectures such as GPUs as memory accesses are local, i.e.
matched nodes don't have to be fetched.

\section{Numerical Experiments}

All experiments were performed with TensorFlow, an open-source library for
numerical computation using data flow graphs, especially suited for deep
learning \cite{abadi_tensorflow_2016}. It features various backends, notably
CUDA to compute on Nvidia GPUs. All computations are carried on an Nvidia Tesla
K40c GPU.

In the sequel, we refer to filters defined by \eqnref{filt_non-param} as
\textit{Non-Param} and filters defined by \eqnref{filt_cheby} as
\textit{Chebyshev}. Filters referred to as \textit{Spline} are defined by
\begin{equation} \label{eq:filt_spline}
	g_\theta(\Lambda) = B \theta,
\end{equation}
where $B \in \R^{n \times K}$ is the cubic B-spline basis and the parameter
$\theta \in \R^K$ is a vector of control points, as proposed by \cite{\bruna}. It is
important to note that, for all methods, we used the Graclus state-of-the-art
coarsening algorithm introduced in \secref{pooling} rather than their "naive
agglomerative method", as they put it. The rational is that we wanted to compare
the filters, not the coarsening algorithms.

We use the following notation when describing network architectures: FC$k$
denotes a fully connected layer with $k$ hidden units, P$k$ denotes a (graph or
classical) pooling layer of size and stride $k$, C$k$ denotes a convolutional
layer with $k$ feature maps and GC$k$ denotes a graph convolutional layer with
$F_{out} = k$ feature maps (with Chebyshev filters if not specified otherwise).
All FC$k$, C$k$ and GC$k$ layers are followed by a ReLU activation $\max(x,0)$.
The final layer is always a softmax regression and the loss energy $E$ is the
cross-entropy loss \todo{with an $\ell_2$ regularization on the weights of all
FC$k$ layers}. Mini-batches are of size $S = 100$.  \todo{dropout}

\todo{$k$-NN similarity graphs are built as follows: }

%\subsection{Revisiting classical CNNs on MNIST}
\subsection{Comparison with classical CNNs on MNIST}
% Struct: motivation, dataset, model, results

To confirm the validity of our model, we first revisit the Euclidean
case, where the graph is a regular grid, on the standard MNIST classification
problem \cite{pro:LeCunBottouBengioHaffner98MNIST}.
This is an important sanity check for our model, which should be able to extract
features on any graph, including the 2D grid graph on which images reside.
\tabref{mnist} indeed shows that our model is able to learn localized filters
and achieve a performance very close to that of a classical CNN with the same
architecture.

\begin{table*}[h!]
\centering
\begin{tabular}{llc}
\toprule
Model & Architecture & Accuracy \\
\midrule
%Linear SVM & 91.76  \\
%Softmax & 92.36  \\
Classical CNN & C32-P4-C64-P4-FC512 & \textbf{99.20} \todo{99.33} \\
Proposed graph CNN & GC32-P4-GC64-P4-FC512 & 98.65 \todo{99.14} \\
\bottomrule
\end{tabular}
\caption{Classification accuracies for the proposed model and a classical CNN on
MNIST.} 
\label{tab:mnist}
\end{table*}

MNIST is a dataset of 70,000 digit numbers represented on a 2D grid of size $28
\times 28$, such that data points lie on a space of $784$ dimensions. To test
our model, we constructed a 8-NN (nearest neighbors) graph of the 2D Euclidean
grid, which produced a graph of $n = |\V| = 976$ nodes (784 pixels and 192 fake
nodes) and $|\E| = 3198$ edges.

The LeNet-5-like network architecture and the following hyper-parameters are
borrowed from the TensorFlow MNIST tutorial\footnote{
\url{https://www.tensorflow.org/versions/r0.8/tutorials/mnist/pros}}: dropout
probability of 0.5, regularization weight of $5\times10^{-4}$, initial learning
rate of 0.03, learning rate decay of 0.95, momentum of 0.9. Convolutional layer
have filters of size $5 \times 5$ while graph convolution layers have the same
support of $K = 5^2 = 25$. All models were trained for \todo{20 epochs}.

\subsection{Text Categorization on 20NEWS}

To demonstrate the usefulness of our model and the versatility of modeling with
graphs, we applied our model to the text categorization problem on 20NEWS.
\tabref{20news} indeed shows that CNNs on graphs are a good model which offers
decent performances. While it does not outperform a simpler algorithm like a
multinomial naive Bayes classifier on this small dataset, it does defeat fully
connected networks, which require much more parameters to be learned. More
importantly, these results show that the statistical assumptions made by the
model are verified.

\begin{table*}[h!] \centering
\begin{tabular}{lc} \toprule
Model & Accuracy \\
\midrule
Linear SVM & 65.90 \\
Multinomial Naive Bayes & 68.51 \\
%Multinomial logistic regression (softmax) & 66.28 \\
Softmax & 66.28 \\
\addlinespace
FC2500 & 64.64 \\
FC2500-FC500 & 65.76 \\
\addlinespace
GC32 & 68.26 \\
GC32-FC500 & \todo{?} \\
\bottomrule \end{tabular}
\caption{Classification accuracies for the proposed model and a other algorithms
on 20NEWS.} 
\label{tab:20news}
\end{table*}

20NEWS consists of 18,846 (11,314 for training and 7,532 for testing) text
documents associated with 20 classes \cite{art:Joachims9620NEWS}. We extracted
the 10,000 most common words from the 93,953 unique words in this corpus. Each
document is represented using the bag-of-words model, normalized across words.
To test our model, we constructed a 16-NN graph of the word2vec
\cite{pro:MikolovChenCorradoDean13word2vec} embedding of those words, which
produced a graph of \todo{$n = |\V| = $ nodes (10k words and ? fake nodes) and
$|\E| = ?$ edges}. The influence of the graph quality is further studied in
\secref{graph_quality}. All models were trained for \todo{20 epochs} by the Adam
optimizer \todo{cite} with a learning rate of 0.1, without regularization nor
dropout. The filter support was set to \todo{$K = 25$}.

\subsection{Definition of spectral filters}

To compare our spectral filters with those proposed in \cite{\bruna}, we measure
the performance of the graph CNN model for each filter definition on the MNIST
and 20NEWS datasets. \tabref{filters} shows that our parametrization outperforms
\cite{\bruna} as well as non-parametric filters, which are not localized and
require $\bO(n)$ parameters to learn, in all cases.

\begin{table*}[h!] \centering
\begin{tabular}{llccc} \toprule
& & \multicolumn{3}{c}{Accuracy} \\
\cmidrule{3-5}
Dataset & Architecture & Non-Param \eqnref{filt_non-param} &
Spline \eqnref{filt_spline} \cite{\bruna} &
Chebyshev \eqnref{filt_cheby} \\
\midrule
MNIST & GC10 & 95.75 & 97.26 & \textbf{97.48} \\
MNIST & GC32-P4-GC64-P4-FC512 & 96.28 & 97.15 & \textbf{98.65} \\
20NEWS & GC32 & \todo{?} & \todo{?} & \todo{?} \\
\bottomrule \end{tabular}
\caption{Comparison of accuracy results for different types of spectral filters.
Spline and Chebyshev filters use \todo{$K=20$}, \todo{$K=5$ for 20NEWS}.} 
\label{tab:filters}
\end{table*}

\subsection{Computational efficiency}

\figref{runtime} indeed demonstrates that the training time for our model scales
as $\bO(n)$ while \cite{\bruna} scales as $\bO(n^2)$. Moreover,
\figref{convergence} gives a sense of how the validation accuracy as well as the
loss energy converges w.r.t. the three filter definitions. Finally,
\tabref{speedup} compares training time on CPU and GPU. The fact that we observe
the same order of speedup as classical CNNs exemplifies the natural
parallelization opportunity offered by our model. That is possible because our
method relies solely on matrix multiplications which are efficiently implemented
by cuBLAS, the linear algebra routines provided by NVIDIA.

\begin{table*}[h!] \centering
\begin{tabular}{llccc} \toprule
& & \multicolumn{2}{c}{Time (ms)} & \\
Model & Architecture & CPU & GPU & Speedup \\
\midrule
Classical CNN & C32-P4-C64-P4-FC512 & 210 & 31 & 6.77x \\
Proposed graph CNN & GC32-P4-GC64-P4-FC512 & 200 & 25 & 8.00x \\
\bottomrule \end{tabular}
\caption{Time to process a mini-batch of 100 MNIST images. That is the total
training time divided by the number of gradient steps.} 
\label{tab:speedup}
\end{table*}

\begin{figure}[h!]
\centering
\includegraphics[width=5cm]{images/speed.png}
\caption{Training time over a mini-batch of 100 20NEWS documents w.r.t. $n$, the
number of selected words, for the three considered filters. The architecture is
GC32 with $K=5$ for Spline and Chebyshev.}
\label{fig:runtime}
\end{figure}

\begin{figure}[h!]
\centering
\subfigure[]{\includegraphics[width=0.49\textwidth]{images/acc.png}}
\hfill
\subfigure[]{\includegraphics[width=0.49\textwidth]{images/loss.png}}
\caption{Plots of accuracy (a) and energy loss (b) for the first 2000
iterations.}
\label{fig:convergence}
\end{figure}

\subsection{Influence of graph quality} \label{sec:graph_quality}

For our method to be successful, his statistical assumptions shall be fulfilled
on the graph where the data resides. The performance, and the quality of the
learned filters, thus critically depends on the quality of the graph. By the
success of classical CNNs, we know these assumptions to be true for images in
Euclidean domains. By our experiments, we know also that a $k$-NN graph of the
grid is good enough (we did not loose much compared to classical CNNs), and the
value of $k$ does not have a strong influence. The extent to which a particular
graph satisfies the assumptions is to compare its performance with a random
graph, which for sure does not. \tabref{mnist_quality} shows that when using a
random graph, the accuracy drops (\todo{to the order of an architecture without
convolution}), as the convolutional layers are not useful anymore to extract
meaningful features. The structure has been lost.

\begin{table*}[h!]
\centering
\begin{tabular}{lcc} \toprule
Architecture & 8-NN on 2D Euclidean grid & random \\
\midrule
GC32 & 97.40 & 96.88 \\
GC32-P4-GC64-P4-FC512 & 98.65 & 95.39 \\
\bottomrule \end{tabular}
\caption{Classification accuracies with different graph constructions on MNIST.} 
\label{tab:mnist_quality}
\end{table*}

While images naturally lie on a graph, the 2D grid, that is not the case for
text documents. For our method to be applied, a feature graph does have to be
built.
We investigate here 5 $16$-NN similarity graphs of words. The simplest option is
to represent each word as its corresponding column in the bag-of-words matrix.
Another approach is to learn embeddings on the corpus with word2vec
\cite{pro:MikolovChenCorradoDean13word2vec} or to use pre-trained embeddings on
Google News (given by the authors). As the dataset gets larger (in number of
samples and dimensionality), it is often not an option to compute the distance
between all features, such that an approximate nearest neighbors algorithm shall
be used. We used the LSHForest \cite{pro:BawaCondieGanesan05LSHForest} on the
learned word2vec embedding. \tabref{20news_quality} reports the classification
results. It shows that the graph built on the learned word2vec embedding is the
best at capturing the local and stationarity properties of text documents. It is
worth noticing that the approximate $k$-NN graph constructed from it is almost
as bad as the random graph, meaning that it may be a better strategy to learn a
good low-dimensional embedding first and then construct an exact $k$-NN graph
from this embedding.

\begin{table*}[h!] \centering
\begin{tabular}{ccccc} \toprule
& \multicolumn{2}{c}{word2vec} & & \\
bag-of-words & pre-trained & learned & approximate & random \\
\midrule
67.50 & \todo{66.98} & 68.26 & 67.86 & 67.75 \\
\bottomrule \end{tabular}
\caption{Classification accuracies of GC32 with different graph constructions on 20NEWS.} 
\label{tab4b}
\label{tab:20news_quality}
\end{table*}

\section{Conclusion and Future Work}

We have introduced an efficient implementation of CNNs on non-Euclidean
manifolds represented by graphs using tools from GSP. Experiments have shown the
ability of the model to extract local and stationary features through the graph
convolutional layers. Compared with the first generation of spectral graph CNNs
introduced in \cite{\bruna}, our model provides a strict control over the local
support of filters, is computationally more efficient (\figref{runtime}) by
avoiding an explicit use of the Graph Fourier basis and experimentally shows a
better test accuracy (\tabref{filters}).

In the end we addressed the three concerns raised by
\cite{art:HenaffBrunaLeCun15DLgraphs}:
\begin{itemize}
\item We introduced a model whose computational complexity is linear with the
	dimensionality of the data (\figref{runtime}).
\item As they showed already, we confirm that the quality of the input graph is
	of paramount importance (\tabref{20news_quality}).  Their idea to learn it
	from data makes a lot of sense. A natural and future approach would be to
	alternate the learning of the NN parameters and the graph, as a virtuous
	circle.
\item We showed that the statistical assumptions of stationarity and
	compositionality made by the model are verified for text documents
	(\tabref{20news}), as long as the graph is well constructed.
\end{itemize}

Future works will investigate two directions. First, we will explore
applications of this generic model to important fields where the data naturally
lie on non-artificially constructed graphs such as social networks, biological
networks, or telecommunication networks. It makes actually more sense to apply
this generalized CNN framework to natural network-based data, rather than
artificially created networks which quality may vary as seen in the experiments.
Second, we will also improve this framework with tools developed in GSP,
including graph wavelet operators
\cite{art:HammondVandergheynstGribonval11GraphWav, art:CoifmanLafon06DifMap,
	pro:GavishNadlerCoifman10GraphHaar, pro:ChenChengMallat14deepHaar,
	pro:RustamovGuibas14deepHaar}, spectral graph coarsening techniques
	\cite{art:ShumanFarajiVandergheynst16PyramTrans}, and improved localization
	properties \cite{pro:TsitsveroBarbarossa15Uncert,
	pro:PasdeloupAlamiGriponRabbat15Uncert,
art:PerraudinRicaudShumanVandergheynst16Uncert}.

% more data, natural graphs (no need to worry about construction, external info)
% data graph vs feature graph

\newpage
\bibliographystyle{plain}
{\setstretch{0} \small
\bibliography{refs}
%\bibliography{bib_nips16_short}
}

\end{document}
